{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_extraction(text_file):\n",
    "    filename = str(text_file)\n",
    "    text_file = open(text_file, \"r\")\n",
    "    text_file_lines = text_file.read().split('\\n')\n",
    "    if text_file_lines[0] == '':\n",
    "        return 0\n",
    "    else:\n",
    "        a = text_file_lines[0].split(',')\n",
    "        if a[2] == 'chainsaw':\n",
    "            return 1\n",
    "        else:\n",
    "            return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9648,
     "status": "ok",
     "timestamp": 1575046134860,
     "user": {
      "displayName": "Forest Fire",
      "photoUrl": "",
      "userId": "18339125348470227546"
     },
     "user_tz": -480
    },
    "id": "2vVp2x11tJgy",
    "outputId": "f6f262da-f769-4bc5-bac6-9d2465754282"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "images_folder = \"images/\"\n",
    "image_list = os.listdir(images_folder)\n",
    "random.seed(10)\n",
    "random.shuffle(image_list)\n",
    "\n",
    "for image in image_list:\n",
    "    samples.append(img_to_array(load_img(images_folder + image, target_size = (200, 200)))) # Change Target Size accordingly to the resolution.\n",
    "\n",
    "    labels.append(label_extraction('txt/'+image[: image.rfind('.png')] + \".txt\"))\n",
    "    \n",
    "labels = to_categorical(labels)\n",
    "        \n",
    "samples = np.array(samples)\n",
    "labels = np.array(labels)\n",
    "\n",
    "eval_x, eval_y = samples[-100:], labels[-100:] # 1000 images each for Normal and Soundscape, 100 in evaluation.\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(samples[:-100], labels[:-100], test_size = 0.2, random_state=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 37469,
     "status": "ok",
     "timestamp": 1575047213397,
     "user": {
      "displayName": "Forest Fire",
      "photoUrl": "",
      "userId": "18339125348470227546"
     },
     "user_tz": -480
    },
    "id": "9mJzkL6UtfHa",
    "outputId": "2af1e885-704d-4817-9619-4320dc642336"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.callbacks import History\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(keras.layers.InputLayer(input_shape=(200, 200, 3)))\n",
    "\n",
    "model.add(keras.layers.convolutional.Conv2D(filters = 16, kernel_size = (3, 3), strides=(1, 1), padding='valid', data_format=\"channels_last\", activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(units=3, input_dim=50,activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "history = History()\n",
    "model.fit(x_train, y_train, batch_size = 32, epochs = 50, callbacks = [history], validation_data = (x_test, y_test), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "def plotResults(): \n",
    "    plt.figure()\n",
    "    N = 50\n",
    "    \n",
    "    # TODO: plot the accuracy/loss variables over training time\n",
    "    plt.plot(np.arange(0, N), history.history[\"accuracy\"], label = \"train_acc\")\n",
    "    plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label = \"val_acc\")\n",
    "\n",
    "    # make the graph understandable: \n",
    "    plt.title(\"Training Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, N), history.history[\"loss\"], label = \"train_loss\")\n",
    "    plt.plot(np.arange(0, N), history.history[\"val_loss\"], label = \"val_loss\")\n",
    "    \n",
    "     # make the graph understandable: \n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1575047220828,
     "user": {
      "displayName": "Forest Fire",
      "photoUrl": "",
      "userId": "18339125348470227546"
     },
     "user_tz": -480
    },
    "id": "nuBZJ72Sy-2n",
    "outputId": "6eb16f6c-b206-43bf-e6d5-75c8c27ec436"
   },
   "outputs": [],
   "source": [
    "model.save_weights('basic-multiclass-model.h5')\n",
    "plotResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import interp\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "def class_report(y_true, y_pred, y_score=None, average='micro'):\n",
    "    if y_true.shape != y_pred.shape:\n",
    "        print(\"Error! y_true %s is not the same shape as y_pred %s\" % (\n",
    "              y_true.shape,\n",
    "              y_pred.shape)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "\n",
    "    if len(y_true.shape) == 1:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    #Value counts of predictions\n",
    "    labels, cnt = np.unique(\n",
    "        y_pred,\n",
    "        return_counts=True)\n",
    "    n_classes = len(labels)\n",
    "    pred_cnt = pd.Series(cnt, index=labels)\n",
    "\n",
    "    metrics_summary = precision_recall_fscore_support(\n",
    "            y_true=y_true,\n",
    "            y_pred=y_pred,\n",
    "            labels=labels)\n",
    "\n",
    "    avg = list(precision_recall_fscore_support(\n",
    "            y_true=y_true, \n",
    "            y_pred=y_pred,\n",
    "            average='weighted'))\n",
    "\n",
    "    metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']\n",
    "    class_report_df = pd.DataFrame(\n",
    "        list(metrics_summary),\n",
    "        index=metrics_sum_index,\n",
    "        columns=labels)\n",
    "\n",
    "    support = class_report_df.loc['support']\n",
    "    total = support.sum() \n",
    "    class_report_df['avg / total'] = avg[:-1] + [total]\n",
    "\n",
    "    class_report_df = class_report_df.T\n",
    "    class_report_df['pred'] = pred_cnt\n",
    "    class_report_df['pred'].iloc[-1] = total\n",
    "\n",
    "    if not (y_score is None):\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        for label_it, label in enumerate(labels):\n",
    "            fpr[label], tpr[label], _ = roc_curve(\n",
    "                (y_true == label).astype(int), \n",
    "                y_score[:, label_it])\n",
    "\n",
    "            roc_auc[label] = auc(fpr[label], tpr[label])\n",
    "\n",
    "        if average == 'micro':\n",
    "            if n_classes <= 2:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                    lb.transform(y_true).ravel(), \n",
    "                    y_score[:, 1].ravel())\n",
    "            else:\n",
    "                fpr[\"avg / total\"], tpr[\"avg / total\"], _ = roc_curve(\n",
    "                        lb.transform(y_true).ravel(), \n",
    "                        y_score.ravel())\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(\n",
    "                fpr[\"avg / total\"], \n",
    "                tpr[\"avg / total\"])\n",
    "\n",
    "        elif average == 'macro':\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([\n",
    "                fpr[i] for i in labels]\n",
    "            ))\n",
    "\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in labels:\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "            # Finally average it and compute AUC\n",
    "            mean_tpr /= n_classes\n",
    "\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "            roc_auc[\"avg / total\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "        class_report_df['AUC'] = pd.Series(roc_auc)\n",
    "        \n",
    "    # print out confusion matrix\n",
    "    print(\"Confusion Matrix: \\n\", confusion_matrix(y_true, y_pred))\n",
    "    return class_report_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "pred_y = model.predict_classes(eval_x, verbose = 1)\n",
    "new_eval_y = tf.argmax(eval_y, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(eval_x, eval_y, verbose = 1)\n",
    "print(f\"Loss: {score[0]}\\nAccuracy: {score[1]}\")\n",
    "print(class_report(new_eval_y, pred_y, y_score=None, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "new.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
